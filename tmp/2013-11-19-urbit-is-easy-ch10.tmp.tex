\chapter{Type Inference}

\emph{Ever, as before, does Madness remain a mysterious-terrific, altogether infernal boiling-up of the Nether Chaotic Deep, through this fair-painted Vision of Creation, which swims thereon, which we name the Real.}

\textbf{(Carlyle)}

\section{On type inference algorithms}

Hoon is a higher-order typed functional language.  Most languages
in this class, Haskell and ML being prominent examples, use
something called the Hindley-Milner unification algorithm.  Hoon
uses its own special sauce instead.

Why?  There are two obvious problems with H-M as a functional
type system, the main one being the wall of heavy-ass mathematics
that greets you instantly when you google it.  We have heard some
claims that H-M is actually quite simple.  We urge all such
claimants to hie themselves to its Wikipedia page, which they'll
surely be able to relieve of its present alarming resemblance to
some string-theory paper in Physics Review D.

(Nor is this in any way an an anti-academic stance.  Quite the
contrary.  Frankly, OS guys really quite seldom find themselves
in the math-department lounge, cadging stray grants by
shamelessly misrepresenting the CAP theorem as a result in
mathematics.  It doesn't seem too much to expect the
mathematicians to reciprocate this basic academic courtesy---how
about it, old chaps?)

Furthermore, besides the drawback that it reeks of math and
programmers who love math are about as common as cats who love a
bath---a problem, but really only a marketing problem---H-M has a
genuine product problem as well.  It's too powerful.  

Specifically, H-M reasons both forward with evaluation, and
backward from constraints.  Pretty unavoidable in any sort of
unification algorithm, obviously.  But since the compiler has to
think both forward and backward, and the programmer has to
predict what the compiler will do, the programmer has to think
backward as well.

Hoon's philosophy is that a language is a UI for programmers, and
the basic test of a UI is to be, ya know, easy.  It is impossible
(for most programmers) to learn a language properly unless they
know what the compiler is doing, which in practice means mentally
stepping through the algorithms it uses (with the exception of
semantically neutral optimizations).  Haskell is a hard language
to learn (for most programmers) because it's hard (for most
programmers) to follow what the Haskell compiler is thinking.

It's true that some programmers have an effective mathematical
intuition that let them ``see'' algorithms without working through
them step by step.  But this is a rare talent, we feel.  And
even those who have a talent don't always enjoy exercising it.

If a thorough understanding of any language demands high-grade
mathematical intuition in its programmers, the language as a UI
is like a doorway that makes you duck if you're over 6 feet tall.
The only reason to build such a doorway in your castle is if you
and all your friends are short, and only your enemies are tall.
Is this really the case here?

Although an inference algorithm that reasons only forward must
and does require a few more annotations from the programmer, the
small extra burden on her fingers is more than offset by the
lighter load on her hippocampus.  Furthermore, programs also
exist to be read.  The modern code monkey is above all things a
replaceable part, and some of these annotations (which a smarter
algorithm might infer by steam) may annoy the actual author of
the code but be a lifesaver for her replacement.

\section{Low-powered type inference}

Broadly speaking, type inference in Hoon has three general
limitations as compared to H-M inference.  

\emph{One}, Hoon really does not think backwards. For instance, it
cannot infer a function's argument type (or to use Hoonese, a
gate's sample type) from its body.  

\emph{Two}, Hoon can infer through tail recursion, but not head
recursion.  It can \emph{check} head recursion, however, given an
annotation.

\emph{Three}, the compiler catches most but not all divergent
inference problems---i.e., if you really screw up, you can put the
compiler into an infinite loop or exponential equivalent.  That's
ok, because an interrupt will still show you your error location.
Also, this never happens once you know what you're doing.

Our experience is that these limitations are minor annoyances at
worst and prudent restrictions at best.  Your mileage may vary.

A good basic test for the power of a type inference algorithm is
whether, given a grammar for a complex AST, it can verify
statically that the noun produced by an LL combinator parser for
the grammar actually fits in the AST type.  (It would be quite
surprising to see any language solve this for an LR parser.)

Hoon indeed checks its own LL parser.  Moreover, the core of
the Hoon compiler---\kode{++ut}, which handles both type inference and 
Nock generation, is less than 2000 lines (and feels a bit bloated
at that, since for obvious reasons it is relatively old code).
Hoon is pretty expressive, but not \kode{that} expressive.  

So we feel Hoon's approach to type inference offers, beneath its
rich Corinthian leather, an unrivalled balance of simplicity and
power.  Still, don't take it out on the freeway till you're
pretty sure you know how to drive it.

\section{Facts to keep in mind}

Type inference is a frightening problem, especially if you've
been exposed to the wall of math.  Your only real problem in
learning Hoon is to learn not to fear it.  Once you work past
this reasonable but entirely unfounded fear of inference, your
Hoon experience will be simple, refreshing and delightful.  So
first, let's talk through a few reassuring facts:

\emph{One}, it's important to remember---having just read about tiles---
that type inference in Hoon \emph{never sees a tile}.  It operates
exclusively on twigs.  All tiles and synthetic twigs are reduced
to natural twigs for the inference engine's benefit.

\emph{Two}, the semantics of Hoon are in \kode{++ut} in \kode{hoon.hoon}, and 
nowhere else.

\emph{Three}, within \kode{++ut}, all the semantics of Hoon are in the call
graph of one arm---\kode{++mint}.  \kode{++mint} has a case for every
natural hoon.  So do \kode{++play} and \kode{++mull}, but their semantics
are redundant with \kode{++mint}.

\emph{Four}, one leg in the sample of \kode{++mint}---\kode{gol}---which looks
for all the world like a mechanism for backward inference, is
not.  It is semantically irrelevant and only exists to get better
localization on error reports.

\emph{Five}, we've already explained what \kode{++mint} does, but let's
repeat it one more time:

When we have a type that describes the subject for the formula
we're trying to generate, as we generate that formula we want to
also generate a type for the product of that formula on that
subject.  So our compiler computes:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
[subject\PYZhy{}type twig] =\PYZgt{} [product\PYZhy{}type formula]
\end{Verbatim}
\end{framed_shaded}
As long as \kode{subject-type} is a correct description of some
subject, you can take any twig and compile it against
\kode{subject-type}, producing a \kode{formula} such that \kode{*(subject
formula)} is a product correctly described by \kode{product-type}.

\kode{++mint} is the gate that maps \kode{[type twig]} to \kode{[type nock]}.
So, if you know \kode{++mint}, you know Hoon.

\emph{Six}, most of the things \kode{++mint} does are obvious.  For
instance, let's quickly run through how \kode{++mint} handles a
\kode{=+} (\kode{tislus}) twig, \kode{[\%tsls p=twig q=twig]}.  This is a 
synthetic twig:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  open
  \PYZca{}\PYZhy{}  twig
  ?\PYZhy{}  gen
    [\PYZpc{}tsls *]  [\PYZpc{}tsgr [p.gen [\PYZti{} 1]] q.gen]
  ==
\end{Verbatim}
\end{framed_shaded}
i.e., \kode{=+(a b)} is \kode{=\textgreater{}([a .] b)}.  We thus turn to the \kode{\%tsgr}
twig in \kode{++mint}.  Simplifying broadly:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  mint
  |=  [sut=type gen=twig]
  \PYZca{}\PYZhy{}  [p=type q=nock]
  ?\PYZhy{}    gen
      [\PYZpc{}tsgr *]
    =+  fid=\PYZdl{}(gen p.gen)
    =+  dov=\PYZdl{}(sut p.fid, gen q.gen)
    [p.dov (comb q.fid q.dov)]
  ==
++  comb
  |=  [mal=nock buz=nock]
  \PYZca{}\PYZhy{}  nock
  ?:  \PYZam{}(?=([0 *] mal) !=(0 p.mal))
    ?:  \PYZam{}(?=([0 *] buz) !=(0 p.buz))
      [\PYZpc{}0 (peg p.mal p.buz)]
    ?:  ?=([2 [0 *] [0 *]] buz)
      [\PYZpc{}2 [\PYZpc{}0 (peg p.mal p.p.buz)] [\PYZpc{}0 (peg p.mal p.q.buz)]]
    [\PYZpc{}7 mal buz]
  ?:  ?=([\PYZca{} [0 1]] mal)
    [\PYZpc{}8 p.mal buz]
  ?:  =([0 1] buz)
    mal
  [\PYZpc{}7 mal buz]
\end{Verbatim}
\end{framed_shaded}
It may not be obvious what \kode{++comb} is doing, but we can guess
that it's composing two Nock formulas, and in particular doing
its best to get back the \kode{\%8} shorthand that we might have
otherwise thrown away by making \kode{=+} a synthetic hoon.

If you understand this, you understand the general problem that
\kode{++mint} is solving.  Now, let's dive into the details.

\section{Pseudolazy evaluation: \%hold}

We'll start by introducing a new type stem, \kode{\%hold} (yes, we are
bringing them in one by one like Gandalf's dwarves):

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  type  \PYZdl{}|  ?(\PYZpc{}noun \PYZpc{}void)
          \PYZdl{}\PYZpc{}  [\PYZpc{}atom p=term]
              [\PYZpc{}cell p=type q=type]
              [\PYZpc{}core p=type q=coil]
              [\PYZpc{}cube p=* q=type]
              [\PYZpc{}face p=term q=type]
              [\PYZpc{}fork p=type q=type]
              [\PYZpc{}hold p=(list ,[p=type q=twig])]
          ==
\end{Verbatim}
\end{framed_shaded}
(There is now only one type stem that's missing---and a
relatively frivolous and unimportant stem at that.)

\kode{\%hold} looks a little bit funky there.  Let's indulge in a
little more unlicensed simplifying, and pretend it was instead

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
[\PYZpc{}hold p=type q=twig]
\end{Verbatim}
\end{framed_shaded}
(The actual \kode{\%hold}, with its list, is just a big \kode{\%fork}; and can
be adequately represented, at least semantically, by a nested
\kode{\%fork} of simple holds like this one.  The reasons it doesn't
work this way are unimportant and we can forget them for now.)

What are the semantics of a \kode{\%hold}?  Simple---this type
delegates to the value, also of course a type, \kode{p:(mint p q)}.
(Once again, in the real \kode{hoon.hoon}, the \kode{++mint} interface
is also slightly different from our caricature here.)

Here we see how infinite types, such as a linked list, can be
constructed.  Hoon, of course, is a strict language (i.e., one
without lazy evaluation, like Haskell) and cannot construct an
infinitely long linked list.  We certainly can describe, as a
\kode{++type}, the infinite set which contains all linked lists---or
all lists of a given item type, e.g., \kode{[p=type q=twig]}.  But this
infinite set must be defined in a very finite noun.

When we traverse this finite noun by expanding \kode{\%hold} stems, we
implicitly produce the illusion of an infinite type---for
instance, one that in the case of a linked list of @tas, was

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
[ \PYZpc{}fork
  [\PYZpc{}cube 0 \PYZpc{}atom \PYZpc{}n]
  [ \PYZpc{}cell
    [\PYZpc{}atom \PYZpc{}tas]
    [ \PYZpc{}fork
      [\PYZpc{}cube 0 \PYZpc{}atom \PYZpc{}n]
      [ \PYZpc{}cell
        [\PYZpc{}atom \PYZpc{}tas]
        [ \PYZpc{}fork
          [\PYZpc{}cube 0 \PYZpc{}atom \PYZpc{}n]
          ... 
\end{Verbatim}
\end{framed_shaded}
and it's turtles all the way down.  (\kode{[\%cube 0 \%atom \%n]} is of
course the null terminator, \kode{\sig }).

Infinite state is not required to produce this sequence ad
infinitum.  In any language, this is essentially what an infinite
list \emph{is}---an indefinite pattern generated from finite state..
But in Hoon, the illusion is produced at the user level, not
hidden in the language semantics.

Perhaps it's not clear why this is superior\ldots{}

Again, you want to define an infinite list.   But your physical
computer is finite.  So, your infinite list must consist of some
kind of generated pattern.  Lots of ways to do this.

What is the right way to manage this finite state?  State of this
kind can be expressed in three forms: (a) the actual data
structure that the pattern contains; (b) a core, which takes this
payload and wraps a pattern generator around it; (c) an
abstraction, which makes the generator indistinguishable from
simple list data.

It seems clear that since (a) can be converted into (b), and (b)
into (c), the best representation is (a).  For instance, (a) is
easy to send over a network, (b) is clunky at best (we really try
to never send nock over the network), (c) is a research project.
In short, lazy evaluation is inherently a leaky abstraction.

\section{How we use \%hold}

There is one advantage of explicit over implicit laziness which
would leave us no choice but to use explicit laziness in \kode{++ut}, 
even if we expressed the same semantics in a language with
built-in implicit laziness (like Haskell).

The advantage is that the explicit pattern state makes a good
search key for an associative array, i.e., \kode{++map} in Hoon.  The
problem of using an infinite expansion as a search key in a lazy
language is one I'd be surprised to see solved, and even more
surprised to see solved well. 

Consider the function computed by \kode{++nest}.  Again simplifying
the interesting core structure of \kode{++ut}, \kode{(nest sut ref)} is yes
if \emph{it can verify that} every noun which is a member of type
\kode{ref} is also a member of type \kode{sut}.  In other words, \kode{++nest}
is a conservative algorithm---it sometimes busts the innocent,
it always nails the guilty.

(\kode{++nest} and everything else in the Hoon type system practice a
policy of pure ``structural equivalence.''  Even \%face is ignored
for compatibility purposes; only noun geometry is considered.  If
it walks like a duck, etc.)

(You can look at \kode{++nest} yourself.  It is a big function for
Hoon: 150 lines, though arranged quite loosely (it should be 
more like 120) with a complex internal core.)

If you look again at \kode{++type} above and forget about \kode{\%hold}, 
it is probably not hard to imagine building a \kode{++nest} for 
\emph{finite} type nouns.  (The problem is not even equivalent to SAT,
because there is a unification stem but no intersection stem.)

But for (logically) infinite types?  Yes---actually, the problem
is straightforward.  But the solution requires us to use \kode{\%hold}
types as search keys.

Essentially, \kode{++nest} can begin by assuming that every \kode{ref} is a
\kode{sut}, then traverse the parallel trees searching for any case
of a \kode{ref} that might conceal a non-\kode{sut}.  In this traverse, we
can simply keep a set of the \kode{[sut ref]} problems we are solving:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
=+  gil=*(set ,[p=type q=type])
\end{Verbatim}
\end{framed_shaded}
Again, \kode{gil} is just the \kode{++nest} stack as a live problem set.

If we descend into another instance of the search tree we're
actually in, i.e., currently searching for exceptions to the
compatibility assumption, there is no reason to restart our
search.  So, we can prune and ignore these recurrences.  And
so, we can write a decent \kode{++nest}.

Every nontrivial algorithm that traverses types keeps one of
these recurrence sets to detect repeating patterns.

This leaves the onus on the programmer to design types that recur
regularly.  An infinite type \emph{cannot} be traversed if the actual
\kode{[\%hold p=type q=twig]} in it does not recur, \emph{with the exact
same type and twig nouns}.  Indeed, doing anything significant
with this type will cause the compiler to hang (requiring an
interrupt, which will tell you where you made this mistake).

Fortunately, doing the right thing here is much easier than doing
the wrong thing.  Why would you roll your own \kode{list}, anyway?
Use the standard tools and everything will work.

\section{Where we use \%hold}

Everywhere.  But more specifically, every time a wing resolves to
an arm, we don't trace into the callee; we make a \kode{\%hold}.
Hence, in practice the type in a \kode{\%hold} is always a \kode{\%core}.

When other typed functional languages construct or define a gate,
lambda, closure, function, etc., they generally compute a ``type
signature'' for the function.  I.e., it accepts \emph{this} and produces
\emph{that}.  You will see things in Hoon gates that look to all the
world like type signatures.  They aren't type signatures.

In Hoon, if the compiler wants to traverse the type a gate
produces, it simply iterates a subject through the AST---the
twig.  In doing so, we start with the subject

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
[\PYZpc{}hold gate\PYZhy{}type gate\PYZhy{}twig]
\end{Verbatim}
\end{framed_shaded}
and iterate into it, reducing as needed, and checking wherever
possible for logic loops, until we find what we want.

Logically this iteration is always the same as \kode{q:(mint p q)},
but in actual practice \kode{++mint} is making a formula we don't
need.  Also, we only need to actually verify correctness when we
generate a formula---not when we traverse pseudolazy types.
Hence, we have the lighter \kode{++play} gate for traversal.

\section{Head and tail recursion}

The \kode{\ket -} (\kode{kethep}, \kode{\%kthp}) twig, 

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
[\PYZpc{}kthp p=tile q=twig]
\end{Verbatim}
\end{framed_shaded}
produces the type \kode{p:(mint sub \sig (bunt al p.gen))} and the formula
\kode{q:(mint sub q.gen)}, checking that \kode{p:(mint sub q.gen)} nests in
\kode{p:(mint sub \sig (bunt al p.gen))}.  

It is difficult to avoid the word ``cast'' when talking about
\kode{\%kthp}, so we relent and use it.  Essentially we are casting the
product of twig \kode{q} to the icon of tile \kode{p}.

(Casting should not be confused with fishing.  \kode{\%kthp} is a purely
static operation with no runtime effect whatsoever.)

It's a good general principle to cast the product of every arm.
First, because \kode{\%face} is ignored by \kode{++nest}, the cast is an
opportunity to get the names right---for example, your arm can
produce \kode{[foo bar]} and cast it at the end to \kode{[p=foo q=bar]}.
The type produced by mere inference on practical code may be
funky, duplicative, poorly named, etc., etc.  Even if it's not,
it's often good to remind the reader what just got made.

Furthermore, when an arm is used in head recursion, casting its
product becomes essential.  Our simplistic inference algorithm
cannot follow head recursion; but it can check that every step in
head recursion is correct, leaving no loophole.  

A key point is that since \kode{++play} does not verify, it does not
need to descend into twig \kode{q} at all.  Thus when we type-check
head recursion, we check that any step is correct, assuming any
further steps are correct---a positive answer is dispositive.

If you don't completely understand (or believe) this, or if you are
not quite sure on the difference between head and tail recursion,
just cast the product of every arm.

\kode{\%kthp} also has an irregular wide form.  \kode{\ket -(@tas foo)} can also
be written

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
`@tas`foo
\end{Verbatim}
\end{framed_shaded}
For example, although Hoon is perfectly capable of inferring that
decrement produces an atom, the decrement in \kode{hoon.hoon} casts
the product of the loop step anyway:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
=+  b=0
|\PYZhy{}  \PYZca{}\PYZhy{}  @
?:  =(a +(b))
  b
\PYZdl{}(b +(b))
\end{Verbatim}
\end{framed_shaded}
Note that this is just a fancy modern arrangement of the classic Hoon 

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
=+  b=0
|\PYZhy{}  
\PYZca{}\PYZhy{}  @
?:  =(a +(b))
  b
\PYZdl{}(b +(b))
\end{Verbatim}
\end{framed_shaded}
I.e., it is only by casting \emph{within} the arm that 

The idiom of ``barhep kethep tile'' is common enough that you
should rarely see a \kode{\textbar{}-} without a \kode{\ket -}.  Especially while
still a beginning Hoon programmer---when in doubt, cast.

\section{Branch analysis}

Type inference wouldn't be very useful or interesting if we
couldn't learn things about our nouns at runtime.  Actually,
we can't even really \emph{use} most nouns without type inference.

For instance, let's make a list:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} =foo `(list ,@)`[1 2 3 4 \PYZti{}]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} foo
\PYZti{}[1 2 3 4]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} :type; foo
\PYZti{}[1 2 3 4]
it(@)
\end{Verbatim}
\end{framed_shaded}
Looks good.  (Our prettyprinter has traversed the \kode{list} type and
matched it to its own list detection heuristic---which is the
only way we can print a list as such, since \kode{++list} is just a
gate and no trace of the word \kode{list} remains in the type.)

So let's try to use it.  What's the first item?

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} i.foo
! \PYZhy{}type.it(@)
! \PYZhy{}find\PYZhy{}limb.i
! find\PYZhy{}fork
! exit
\end{Verbatim}
\end{framed_shaded}
Doh!  We got a \kode{find-fork} because, with the cast, we threw away
our knowledge that we had \kode{[1 2 3 4 \sig ]} and replaced it with a
repeating structure that is either \kode{\sig } or \kode{[i=@ t=(list ,@)]}.
We cannot pull \kode{i} from \kode{foo}, because we don't know that \kode{foo}
is not \kode{\sig }.

Since we've lost that information, we need a way to test for it
dynamically, and change the type of \kode{foo} on either side of the
branch.  To wit:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} ?\PYZti{}(foo 42 i.foo)
1
\end{Verbatim}
\end{framed_shaded}
What is \kode{?\sig }---\kode{wutsig}, \kode{\%wtsg}?  A simple synthetic that
use our fishing hoon, \kode{?=} (\kode{wuttis}):

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} ?:(?=(\PYZti{} foo) 42 i.foo)
1
\end{Verbatim}
\end{framed_shaded}
On the \kode{\textbar{}} side of the \kode{?:}, we know that \kode{foo} is not \kode{\sig }---and because it's not \kode{\sig }, it has to be a cell.  We can see the
type system learning this: 

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} :type; ?\PYZti{}(foo !! foo)
[i=1 t=\PYZti{}[2 3 4]]
[i=@ t=it(@)]
\end{Verbatim}
\end{framed_shaded}
Notice also the use of \kode{!!}.  \kode{!!} always crashes---so it
produces the type \kode{\%void}.  We're essentially asserting that
\kode{foo} is a non-empty list.   A branch that produces \kode{\%void} will
never return a value (if taken, it will always crash), so its
product can be ignored.  If we changed that:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} :type; ?\PYZti{}(foo \PYZpc{}foobar foo)
[i=1 t=\PYZti{}[2 3 4]]
\PYZob{} \PYZpc{}foobar [i=@ t=it(@)] \PYZcb{}
\end{Verbatim}
\end{framed_shaded}
It's important to note that the \emph{only} hoons recognized in branch
analysis are \kode{?=} (\kode{wuttis}, \kode{\%wtts}, fish), \kode{?\&} (\kode{wutpam},
\kode{\%wtts}, logical and), and \kode{?\textbar{}} (\kode{wutbar}, \kode{\%wtbr}, logical or).
(Boolean logic is fully understood within the test itself, so the
second twig in a \kode{?\&} can depend on the result of the first.)

Of course, synthetic hoons that reduce to these work as well.
However, we don't learn from any other test, even if we could:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} ?:(?=(\PYZti{} foo) 42 i.foo)
1
\PYZti{}waclux\PYZhy{}tomwyc/try=\PYZgt{} ?:(=(\PYZti{} foo) 42 i.foo)
! \PYZhy{}type.it(@)
! \PYZhy{}find\PYZhy{}limb.i
! find\PYZhy{}fork
! exit
\end{Verbatim}
\end{framed_shaded}
Type inference in Hoon is anything but magic.  It's a relatively
simple algorithm, which you nonetheless need to understand if
you're going to make it as a Hoon programmer.  ``It just works''
and ``do what I mean'' are not in the building.

All branches (such as \kode{?\sig }) reduce to \kode{?:}.  It would appear, for
instance, that Hoon has (like many other functional languages)
pattern-matching primitives, \kode{?-} (and \kode{?+}, which takes a
default).  Au contraire---these are just synthesized from \kode{?:}
and \kode{?=}.

Finally, another thing that branch analysis can do is statically
detect that branches aren't taken.  For instance, with our little
\kode{foo} list, we cannot know \emph{statically} whether or not it's null,
but we can test it \emph{dynamically}.  Statically, however, we know
one thing---\kode{foo} cannot have the value \kode{\%foobar}.

So what happens if we fish for it?

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}zod/try=\PYZgt{} ?:(?=(\PYZpc{}foobar foo) 42 i.foo)
! mint\PYZhy{}vain
! exit
\end{Verbatim}
\end{framed_shaded}
For lo, we fish in vain.  We get this error whenever a branch is
not taken.  This is tremendously useful when, for example, \kode{?-}
switches on a kelp---if we write a case for a stem that doesn't
exist, or miss a stem that must be handled, the compiler knows.

\section{Geometric and generic polymorphism}

One interesting question in any language is what happens when you
pass a function an argument more specialized than its declaration.

Consider a function that takes a pair \kode{[a b]} and produces [b a].
Any noun will obviously do for \kode{b} and \kode{a}.  So the sample tile
is \kode{[a=\textbackslash{}* b=\textbackslash{}*]}.  But suppose the caller knows something more
about \kode{a} and \kode{b}.  What happens?  Let's give it a try:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}zod/try=\PYZgt{} :type; (|=([a=* b=*] [b a]) \PYZpc{}foo \PYZpc{}bar)
[7.496.034 7.303.014]
[* *]
\end{Verbatim}
\end{framed_shaded}
With \kode{\textbar{}=} (\kode{bartis}, \kode{\%brts}), the normal way of building a gate,
we lost all our type information.  Sometimes this is fine.
Sometimes it's exactly what we want.  But sometimes\ldots{}

Here's something else we can do:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}zod/try=\PYZgt{} :type; (|*([a=* b=*] [b a]) \PYZpc{}foo \PYZpc{}bar)
[\PYZpc{}bar \PYZpc{}foo]
[\PYZpc{}bar \PYZpc{}foo]
\end{Verbatim}
\end{framed_shaded}
By using \kode{\textbar{}\textbackslash{}*} (\kode{bartar}, \kode{\%brtr}), we seem to have produced the
same noun, but not lost any type information.  Interesting\ldots{}

The difference is that while both \kode{\textbar{}=} and \kode{\textbar{}\textbackslash{}*} are polymorphic,
\kode{\textbar{}=} uses \emph{geometric} polymorphism and \kode{\textbar{}\textbackslash{}*} uses \emph{generic}.
Which should you choose?

There's a simple rule for this, which is that unless you are a
wizard doing wizardly things, you should use \kode{\textbar{}=}.  Generic
polymorphism is a way to build and use tools like containers
(lists, etc), which in other, inferior languages might simply be
built in to the language semantics.  Especially as a new Hoon
programmer, you are probably not building heavy machinery of this
kind, and should probably make do with the standard components.

But you will be \emph{using} containers, etc.  So there's no shortcut
for understanding both systems.

\subsection{Geometric polymorphism}

We've already met the fundamental function of geometric
polymorphism, \kode{++nest}.  The question \kode{(nest sut ref)} asks is:
can I take any noun in type \kode{ref}, and use it as if it was within
\kode{sut}?

Consider the trivial example above---the question is, can we use
our noun [\%foo \%bar] as if it was a \kode{[a=\textbackslash{}* b=\textbackslash{}*]}?  

When we use \kode{\%=} (\kode{centis}, \kode{\%cnts}) to modify the sample in a
core, we actually change the type of the core.  (Of course we are
not \emph{modifying} the core per se, but creating a new one with the
given changes.)

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
[ \PYZpc{}core
  [ \PYZpc{}cell 
    [\PYZpc{}cell [\PYZpc{}face \PYZpc{}a \PYZpc{}noun] [\PYZpc{}face \PYZpc{}b \PYZpc{}noun]]
    context
  ]
  (map term twig)
]
\end{Verbatim}
\end{framed_shaded}
to

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
[ \PYZpc{}core
  [ \PYZpc{}cell 
    [\PYZpc{}cell [\PYZpc{}cube \PYZpc{}foo [\PYZpc{}atom \PYZpc{}tas]] [\PYZpc{}cube \PYZpc{}bar [\PYZpc{}atom \PYZpc{}tas]]]
    context
  ]
  (map term twig)
]
\end{Verbatim}
\end{framed_shaded}
This poses a couple of problems.  First, remember, when we infer
into a gate call or any other use of a core arm, we infer lazily
by creating a \kode{[\%hold type twig]} with the subject core and the
arm body.  When we want to find out what type the arm produces,
we simply step through it.

But among other possible problems here, we've actually \emph{destroyed
our argument names}.  If we try to step through the body of the
function with this modified core, we can't possibly play \kode{[b a]}.
The names \kode{a} and \kode{b} simply won't resolve.

This is why a core is actually more complicated than our original
explanation would suggest.  The \kode{\%core} frond is actually:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  type  \PYZdl{}\PYZpc{}  [\PYZpc{}core p=type q=coil]
          ==
++  coil  \PYZdl{}:  p=?(\PYZpc{}gold \PYZpc{}iron \PYZpc{}lead \PYZpc{}zinc)
              q=type
              r=[p=?(\PYZti{} \PYZca{}) q=(map term foot)]
          ==
++  foot  \PYZdl{}\PYZpc{}  [\PYZpc{}ash p=twig]
              [\PYZpc{}elm p=twig]
          ==
\end{Verbatim}
\end{framed_shaded}
Geeze, man, what is all this nonsense?  Okay, fine.  Let's
explain the real \kode{\%core}.

First, \kode{q} in the \kode{coil} is the original payload type for use in
a \kode{\%hold}.  Hence geometric polymorphism.  The question we have
to answer whenever we use an arm is: is this core corrupt?  As
in: is the payload that's in it now geometrically compatible with
the payload that its formulas were compiled with?  Can we use the
present payload as if it was the original, default payload?

It is actually not a type error of any kind to produce a modified
core with the payload set to any kind of garbage at all.  It's
just a type error to \emph{use} it---unless the payload is compatible.
And when we use a geometric arm, we test this compatibility and
then treat the present sample as if it was the original.

\kode{q.r} in the coil is the map of arms.  The polymorphism model is
not an attribute of the core, but of the arm---\kode{\%ash} means
geometric, \kode{\%elm} means generic.  

\kode{p.r}, if not null, is the actual generated battery of Nock
formulas.  Why do we need this at compile time?  Remember
\kode{\%ktsg}, which folds constants.  

\kode{p.r} is null while we are compiling the core's own arms, for
obvious reasons (though we could get a little smarter about
circularities), but once we complete this task we can put the
battery in the type.  The result is that, if we are building a
reef of cores, we can fold arms of this core in the next one
down.  

For example, one of the main uses of \kode{\%ktsg} is simply to make
default samples constant, so that we don't have to perform some
simple but unnecessary computation every time we use a gate.
Because we can only fold an arm in a completed core, a good
general practice in building applications is to use a reef of at
least two cores---inner for the tiles, outer for the functions.

All that remains is this mysterious \kode{p} field.  If you are an OO
geek of a certain flavor who was once busted with a Sharpie for
writing ``BERTRAND MEYER IS GOD'' on a Muni bus, you may be
familiar with the broad language concept of \emph{variance}.

Polymorphism in Hoon supports four kinds of variance: \kode{\%gold}
(invariant), \kode{\%lead} (bivariant), \kode{\%zinc} (covariant), and
\kode{\%iron} (contravariant).

The question of variance arises when we want to test whether one
core is compatible with another.  Hoon is a functional language,
for example, so it would be nice to pass functions around.  Gosh,
even C can pass functions around.

For core A to nest within core B (any A can be used as a B), it
seems clear that A should have the same set of arms as B (with
formulas at exactly the same axes in the battery), and that any
product of an A arm should nest within the product of the
corresponding B arm.

But what about the payloads?  Worse---what about the \emph{contexts}?
A simple rule might be that the payload of A must also nest
within the payload of B.  Unfortunately, this doesn't work.

Suppose, for instance, that I write a sort gate, one of whose
arguments is a comparison gate producing a loobean.  (This is
generally a problem calling for generic polymorphism, but let's
assume we're sorting lists of a fixed type.)  Okay, great.  We
know the \emph{sample} type of the comparator---but what about the
\emph{context}?  The sort library cannot possibly have the same
context as the application which is using the sort library.  So,
the cores will be incompatible and the invocation will fail.

This rule, which doesn't work in this case, is the rule for
\kode{\%gold} (invariant) cores.  Every core is created \kode{\%gold}, and
remains \kode{\%gold} from the perspective of its own arms.

But the type of some arbitrary comparator, which is an argument
to our sort function, cannot possibly be \kode{\%gold}.  Rather, we
need an \kode{\%iron} (contravariant) core.  You can turn any \kode{\%gold}
core into an \kode{\%iron} one with \kode{\ket \textbar{}} (\kode{ketbar}, \kode{\%ktbr}), but the
transformation is not reversible.

The rules for using an \kode{\%iron} core are that (a) the context is
opaque (can neither be read nor written), and (b) the sample is
write-only.  Why?  Because it's absolutely okay to use as your
comparator a gate which accepts a more general sample than you'll
actually call it with.  You can write a more specialized noun
into this sample---but if you read the default value and treat it
as more specialized, you have a type loophole.

A \kode{\%zinc} (covariant) core is the opposite---the context remains
opaque, the sample is read-only.  We don't use any \kode{\%zinc} at
present, but this is only because we haven't yet gotten into
inheritance and other fancy OO patterns.  (Hoon 191 had
inheritance, but it was removed as incompletely baked.) You make
a \kode{\%gold} core \kode{\%zinc} with \kode{\ket \&} (\kode{ketpam}, \kode{\%ktpm}).

Finally, the entire payload of a \kode{\%lead} (invariant) core is
immune to reading or writing.  So all that matters is the product
of the arms.  You make a lead core with \kode{\ket ?} (\kode{ketwut}, \kode{\%ktwt}).

\subsection{Generic polymorphism}

When the arm we're executing is \kode{\%elm}, not \kode{\%ash}, there is
actually \emph{no} check that the payload in the actual core, type
\kode{p}, nests in the original payload \kode{q.q}.

Moreover, \kode{\%elm} arms (defined not with \kode{++} or \kode{slus}, which
means \kode{\%ash}, but \kode{+-} or \kode{shep}) are actually \emph{not even
type-checked when compiled}.  They have to generate valid nock,
but all verification is disabled.

Why?  How?  Because there's another way of dealing with a core
whose payload has been modified.  In a sense, it's incredibly
crude and hacky.  But it's perfectly valid, and it lets Hoon
provide the same basic feature set provided by generic features
in other languages, such as Haskell's typeclasses.

Logically, when we use an \kode{\%elm}, generic or ``wet'' arm, we simply
\emph{recompile the entire twig} with the changed payload.  If this
recompilation, which \emph{is} typechecked, succeeds---and if it
\emph{produces exactly the same Nock formula}---we know that we can
use the modified core as if it was the original, without of
course changing the static formula we generated once.

Why does this work out to the same thing as a typeclass?  Because
with a wet core, we are essentially just using the source code of
the core as a giant macro in a sense.  Our only restriction is
that because the Nock formula that actually executes the code
must be the same formula we generated statically for the battery.

In a sense this defines an implicit typeclass: the class of types
that can be passed through the arm, emerging intact and with an
identical formula.  But no declaration of any sort is required.
You could call it ``duck typeclassing.''

Of course, a lot of caching is required to make this compile with
reasonable efficiency.  But computers these days are pretty fast.

This description of how wet arms work is not quite correct,
though it's the way ancient versions of Hoon worked.  The problem
is that there are some cases in which it's okay if the modified
core generates a different battery---for example, if the original
battery takes branches that are not taken in this specific call.

So we have a function \kode{++mull} which tests whether a twig
compiled with one subject will work with another.  But still,
thinking of the wet call check as a simple comparison of the
compiled code is the best intuitive test.

Again, your best bet as a novice Hoon programmer is to understand
that this is how things like \kode{list} and \kode{map} work, that someone
else who knows Hoon better than you wrote these tools, and that
in general if you want to use generic polymorphism you're
probably making a mistake.  But if you wonder how \kode{list} works---this is how it works.

\subsection{Generic polymorphism action sequences}

Let's deploy this boy!  Here is \kode{++list}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  list  |*(a=\PYZus{},* \PYZdl{}|(\PYZti{} [i=a t=(list a)]))
\end{Verbatim}
\end{framed_shaded}
Don't worry.  This is just grievously badass hardcore Hoon---as
irregular as Marseilles street slang.  As a novice Hoon monkey,
you won't be writing \kode{++list} or anything like it, and you can
stick to French as she is spoke in Paris. 

On the other hand, at least this grizzled old baboon has no
trouble parsing the apparent line noise above.  Why so funky?
Why, oh why, \kode{\_,*}?  Because for various irrelevant reasons,
the \kode{++list} here is trying as hard as possible to build itself
out of tiles.

For instance, in general in Hoon it is gauche for a gate to use
its core's namespace to recurse back into itself, but tiles do
not expose their own internals to the twigs they contain
(otherwise, obviously, they could not be hygienic).

manipulate the subject such that it's not possible to loop
properly.  So in fact there is no alternative but to use \kode{(list
a)} within \kode{++list}---a normal usage in complex tiles, but only
in complex tiles.

But a normal person wouldn't use tiles to prove a point.  They'd
do it like this---let's use the REPL to build a \kode{list} replacement, 
without one single ``obfuscated Hoon trick.''

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
\PYZti{}zod/try=\PYZgt{} =lust |*(a=\PYZdl{}+(* *) |=(b=* ?@(b \PYZti{} [i=(a \PYZhy{}.b) t=\PYZdl{}(b +.b)])))
\PYZti{}zod/try=\PYZgt{} ((lust ,@) 1 2 \PYZti{})
\PYZti{}[1 2]
\PYZti{}zod/try=\PYZgt{} ((list ,@) 1 2 \PYZti{})
\PYZti{}[1 2]
\PYZti{}zod/try=\PYZgt{} `(list ,@)``(lust ,@)`[1 2 \PYZti{}]
\PYZti{}[1 2]
\end{Verbatim}
\end{framed_shaded}
Note that \kode{list} and \kode{lust} do the same thing and are perfectly
compatible.  But sadly, \kode{lust} still looks like line noise.
Let's slip into something more comfortable:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
|*  a=\PYZdl{}+(* *)
|=  b=*
?@  b  \PYZti{}
[i=(a \PYZhy{}.b) t=\PYZdl{}(b +.b)]
\end{Verbatim}
\end{framed_shaded}
We haven't met \kode{\$+} (\kode{buclus}) yet.  \kode{\$+(p q)} is a tile for a
gate which accepts \kode{p} and produces \kode{q}.  The spectre of function
signatures once again rears its ugly head---but \kode{\$+(p q)} is no
different from \kode{\$\_(\textbar{}+(p \_q))}.

Otherwise, when we think of a wet gate (\kode{\textbar{}*}) as a macro, we see
(list ,@) producing

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
|=  b=*
?@  b  \PYZti{}
[i=(,@ \PYZhy{}.b) t=\PYZdl{}(b +.b)]
\end{Verbatim}
\end{framed_shaded}
This function is easily recognized as a gate accepting a noun and
producing a list of atoms---in short, perfectly formed for an
herbaceous tile.  Pass it some random noun off the Internet, and
it will give you a list of atoms.  In fact, we could define it
on its own as:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  atls
  |=  b=*
  ?@  b  \PYZti{}
  [i=(,@ \PYZhy{}.b) t=\PYZdl{}(b +.b)]
\end{Verbatim}
\end{framed_shaded}
and everywhere you write \kode{(list ,@)}, you could write \kode{atls}.
Which would work perfectly despite its self-evident lameness.
Note \emph{in particular} that the inner \kode{b} gate is \emph{not} wet, but
rather dry---once we have done our substitution, there is no need
to get funky.

Let's do some little fun thing with \kode{list}---e.g., gluing two lists
together:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  weld
  \PYZti{}/  \PYZpc{}weld
  |*  [a=(list) b=(list)]
  =\PYZgt{}  .(a \PYZca{}.(homo a), b \PYZca{}.(homo b))
  |\PYZhy{}  \PYZca{}+  b
  ?\PYZti{}  a  b
  [i.a \PYZdl{}(a t.a)]
\end{Verbatim}
\end{framed_shaded}
What is this \kode{homo} thing (meaning ``homogenize'', of course)?
Exactly that---it homogenizes the type of a list, producing its
sample noun unchanged:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  homo
  |*  a=(list)
  \PYZca{}+  =\PYZlt{}  \PYZdl{}
    |\PYZpc{}  +\PYZhy{}  \PYZdl{}  ?:(\PYZus{}? \PYZti{} [i=(snag 0 a) t=\PYZdl{}])
    \PYZhy{}\PYZhy{}
  a
\end{Verbatim}
\end{framed_shaded}
Here is more dark magic that should clearly be ignored.  But
essentially, to homogenize a list \kode{a}, we are casting \kode{a} to a
deranged pseudo-loop that produces an infinite stream of the
first item in \kode{a}, selected via the \kode{++snag} function:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
++  snag
  \PYZti{}/  \PYZpc{}snag
  |*  [a=@ b=(list)]
  |\PYZhy{}
  ?\PYZti{}  b
    \PYZti{}|(\PYZsq{}snag\PYZhy{}fail\PYZsq{} !!)
  ?:  =(0 a)
    i.b
  \PYZdl{}(b t.b, a (dec a))
\end{Verbatim}
\end{framed_shaded}
\kode{++snag} of course selects any item in the list; but if \kode{b} has a
type more complex than a homogeneous list (e.g., the type system
might well know the number of items, etc., etc), Hoon is nowhere
near enough to see that the counter is always 0.  So the type
produced by \kode{++snag} is the union of all list elements, which is
precisely the type we want for our homogenized list.

As for \kode{\ket .} (\kode{ketdot}, \kode{\%ktdt}), we can see it in \kode{++open}:

\begin{framed_shaded}
\begin{Verbatim}[fontsize=\relsize{-2.5},fontseries=b,commandchars=\\\{\}]
  ++  open
    \PYZca{}\PYZhy{}  twig
    ?\PYZhy{}  gen
      [\PYZpc{}ktdt *]  [\PYZpc{}ktls [\PYZpc{}cnhp p.gen q.gen \PYZti{}] q.gen]
    ==
\end{Verbatim}
\end{framed_shaded}
I.e., \kode{\ket .(a b)} is \kode{\ket +((a b) b)} is \kode{\ket -(\_(a b) b)}.

\kode{++weld} prudently casts its product to the type of the base list
\kode{b}.  In future this \kode{\ket +}  will probably be removed, since we are
perfectly capable of inferring that when you weld \kode{(list a)} and
\kode{(list b)}, you get a \kode{(list ?(a b))}.  But there's old code this
change might confuse.

In short: generic polymorphism is cool but wacky.  Leave it to
the experts, please!